# -*- coding: utf-8 -*-
"""NeuralNetwork.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oucAnF0jKYvknfeGrKNEPEoVG264ag5p

1.) Build a Neural Network model for 50_startups data to predict profit
"""

# import library

import numpy as np # mathematical and statistical functions
import pandas  as pd # data manipulation
import matplotlib .pyplot as plt # visualization
import seaborn as sns # advanced visualization
import io # importing  external files like csv, excel
import statsmodels.api as sm # stastical model (qq plot)
from scipy.stats import kurtosis, skew
from scipy import stats
from sklearn  import preprocessing
from sklearn.model_selection import train_test_split # # data partioning
from  keras.models import Sequential # neural network
from keras.layers import InputLayer, Dense # neural network

# Import data

startup = pd.read_csv("50_Startups.csv")
startup

startup .info()
startup.shape

#EDA
startup.describe()

# Graphical Representation

# For R&D Spent
skew(startup["R&D Spend"])
plt.hist(startup["R&D Spend"],edgecolor = 'black')
sns.distplot(startup["R&D Spend"], hist=True,  
             color = 'darkblue', 
             hist_kws={'edgecolor':'black'})

sm.qqplot(startup["R&D Spend"], line='s')

# For Administration   


plt.hist(startup["Administration"],edgecolor = 'black')
sns.distplot(startup["Administration"], hist=True,  
             color = 'darkblue', 
             hist_kws={'edgecolor':'black'})

sm.qqplot(startup["Administration"], line='s')

skew(startup["Administration"])

# For  Marketing Spend


plt.hist(startup["Marketing Spend"],edgecolor = 'black')
sns.distplot(startup["Marketing Spend"], hist=True,  
             color = 'darkblue', 
             hist_kws={'edgecolor':'black'})

sm.qqplot(startup["Marketing Spend"], line='s')

skew(startup["Marketing Spend"])

# For Profit

plt.hist(startup["Profit"],edgecolor = 'black')
sns.distplot(startup["Profit"], hist=True,  
             color = 'darkblue', 
             hist_kws={'edgecolor':'black'})

sm.qqplot(startup["Profit"], line='s')

skew(startup["Profit"])

# For State

sns.countplot(x="State", data=startup)
#sns.barplot(x="State", data= startup["State"])

"""**To find the association between continous variables and continous variables we do scatter plot and coreatiaon matrix**"""

# Correlation


corematrix = startup.corr()
corematrix
sns.pairplot(startup, hue="State")

f, ax = plt.subplots(figsize =(9, 8)) 
sns.heatmap(corematrix, ax = ax, cmap ="YlGnBu", linewidths = 0.1, annot = True)

"""**To find the association between categorical input variables and continous output variables  taken the help oof box plot and Anova test**"""

sns.boxplot(x="State", y= "Profit", data= startup)

# One way anova or F-test

df_anova = startup[['Profit','State']]
df_anova
grps = pd.unique(df_anova.State.values)
grps
d_data = {grp:df_anova['Profit'][df_anova.State == grp] for grp in grps} # This will  do grouping of all values  based on the states categories
d_data
F, p = stats.f_oneway(d_data['California'], d_data['Florida'], d_data['New York'])
print("p-value for significance is: ", p)

# Anova test

import statsmodels.api as sm
from statsmodels.formula.api import ols

model = ols('Profit ~ State',                 # Model formula
            data = startup).fit()
                
anova_result = sm.stats.anova_lm(model, typ=2)
print (anova_result)

# Missing value treatment 

startup.isna().sum()

# Label Encoding

label_encoder = preprocessing.LabelEncoder()
# Encode labels in column 'State'. 
startup['State']= label_encoder.fit_transform(startup['State']) 
startup

# Normalization

normalized_startup = pd.DataFrame(preprocessing.normalize(startup[["R&D Spend", "Administration", "Marketing Spend" ,"Profit","State"]]))
normalized_startup.columns = ["R&D Spend", "Administration", "Marketing Spend" ,"Profit","State"]
normalized_startup
normalized_startup.describe()
normalized_startup.dtypes

# Data Partioning 

x=normalized_startup[["R&D Spend",	"Administration","Marketing Spend", "State"]]
y= normalized_startup["Profit"]
x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.7,test_size=0.3,random_state=123)

x_train.shape, y_train.shape, x_test.shape,y_test.shape

"""# Model Building --- Neural Network
# Building the architecture ****
"""

# Defining the  input layers


# number of input nurons = 4 since four input features
input_neurons = x_train.shape[1]
input_neurons

# Defining the output layers

output_neurons=1

# Defining hidden layers
no_hidden_layers=2
no_neurons_hl1= 10
no_neurons_hl2= 5

# Defining the architecture of the model
model =  Sequential()
model.add(InputLayer(input_shape =(input_neurons,))) # input
model.add(Dense(units=no_neurons_hl1,kernel_initializer='normal', activation="relu")) # hidden layer1
model.add(Dense(units=no_neurons_hl2, kernel_initializer='normal',activation="relu")) # hidden layer2
model.add(Dense(units=output_neurons,kernel_initializer='normal'))  # output

#Summary model

model.summary()

# number of parameters between first input layers and first hidden layers
input_neurons * no_neurons_hl1 +10 # 10 is bias

# number of parameters between  first hidden layers and second hidden layers
no_neurons_hl1 * no_neurons_hl2 +5  # 10 is bias

# number of neurons between second hidden layers and output layers

no_neurons_hl2 * output_neurons+1

# Compiling the model
model.compile(loss="mean_squared_error", optimizer="adam",metrics=["mse"])

# Training the model
# epoch = 30

model_history = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=30)

# Evaluating the model performance on test data

predict =  model.predict(x_test)
predict = pd.DataFrame(predict)
predict

from sklearn.metrics import mean_squared_error
from math import sqrt

rmse = sqrt(mean_squared_error(y_test, predict))
rmse

#visualizing the model performance
# summerise history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title ("Model Loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend(["train","validation"],loc="upper right")
plt.show

# summerize history for mse
plt.plot(model_history.history['mse'])
plt.plot(model_history.history['val_mse'])
plt.title ("Model MSE")
plt.ylabel("MSE")
plt.xlabel("Epoch")
plt.legend(["train","validation"],loc="upper right")
plt.show

"""##  Visualization"""

#!pip install ann_visualizer   # required to install ann_visulizer library

#from ann_visualizer.visulize import ann_viz
from ann_visualizer.visualize import ann_viz
from graphviz import Source
ann_viz(model, title="The neural network model of startup_proft",view=True)
graph_source = Source.from_file('network.gv')
graph_source

print(graph_source.source)

model.weights

"""**2. Predict the burned area of forest fires with neural networks**"""

forest_fires = pd.read_csv("forestfires.csv")
forest_fires

forest_fires.dtypes

forest_fires.info

forest_fires.describe()

"""Explotary data analysis"""

forest_fires.isna().sum()

# Corealtion matrix
corematrix = forest_fires.corr()
corematrix

sns.pairplot(forest_fires, hue="size_category")

# heat map of corelation coefficients 


f, ax = plt.subplots(figsize =(25, 15)) 
sns.heatmap(corematrix, ax = ax, cmap ="YlGnBu", linewidths = 0.1, annot = True)

# Converting  size_category variable into numerical varible


forest_fires.size_category[forest_fires.size_category == 'small'] = 0
forest_fires.size_category[forest_fires.size_category == 'large'] = 1
forest_fires
forest_fires.dtypes

forest_fires =  forest_fires.drop(["month","day"], axis =1)
forest_fires

forest_fires.columns

#Normalization 


normalized_forest_fires= pd.DataFrame(preprocessing.normalize(forest_fires[['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain', 'area', 'dayfri', 'daymon', 'daysat', 'daysun', 'daythu', 'daytue', 'daywed', 'monthapr', 'monthaug', 'monthdec', 'monthfeb', 'monthjan', 'monthjul','monthjun', 'monthmar', 'monthmay', 'monthnov', 'monthoct', 'monthsep']]))
       
normalized_forest_fires.columns = ['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain', 'area',
       'dayfri', 'daymon', 'daysat', 'daysun', 'daythu', 'daytue', 'daywed',
       'monthapr', 'monthaug', 'monthdec', 'monthfeb', 'monthjan', 'monthjul',
       'monthjun', 'monthmar', 'monthmay', 'monthnov', 'monthoct', 'monthsep']
normalized_forest_fires["size_category"] = forest_fires["size_category"]
normalized_forest_fires["size_category"]=  normalized_forest_fires["size_category"].astype(int)
normalized_forest_fires.describe()
normalized_forest_fires.dtypes

normalized_forest_fires.describe

# Data Partioning
normalized_forest_fires
x=normalized_forest_fires[['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain', 'area',
       'dayfri', 'daymon', 'daysat', 'daysun', 'daythu', 'daytue', 'daywed',
       'monthapr', 'monthaug', 'monthdec', 'monthfeb', 'monthjan', 'monthjul',
       'monthjun', 'monthmar', 'monthmay', 'monthnov', 'monthoct', 'monthsep']]
y= normalized_forest_fires["size_category"]
x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.7,test_size=0.3,random_state=123)

x_train.shape, y_train.shape, x_test.shape,y_test.shape

"""**Model Building**"""

# Model  architecture

# Defining the  input layers


# number of input nurons = 4 since four input features
input_neurons = x_train.shape[1]
input_neurons

# Defining the output layers

output_neurons=1

# Defining hidden layers
no_hidden_layers=2
no_neurons_hl1= 10
#no_neurons_hl2= 5

# Defining the architecture of the model
model =  Sequential()
model.add(InputLayer(input_shape =(input_neurons,))) # input
model.add(Dense(units=no_neurons_hl1, activation="relu")) # hidden layer1
#model.add(Dense(units=no_neurons_hl2, activation="relu")) # hidden layer2
model.add(Dense(units=output_neurons,activation ='sigmoid'))  # output

# summary Model
model.summary()

# number of parameters between first input layers and first hidden layers
input_neurons * no_neurons_hl1 +10 # 10 is bias

# number of neurons between second hidden layers and output layers

no_neurons_hl1 * output_neurons+1

# Compiling the model
model.compile(loss="binary_crossentropy", optimizer="Adam",metrics=["Accuracy"])

# Training the model
# epoch = 30

model_history = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=30)

# Evaluating the model performance on test data

predict =  model.predict_classes(x_test)
predict = pd.DataFrame(predict)
predict.columns
predict

from sklearn.metrics import mean_squared_error
from math import sqrt

rmse = sqrt(mean_squared_error(y_test, predict))
rmse

#visualizing the model performance
# summerise history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title ("Model Loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend(["train","validation"],loc="upper right")
plt.show

# summerize history for mse
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title ("Model MSE")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.legend(["train","validation"],loc="upper right")
plt.show

ann_viz(model, title="The neural network model of startup_proft",view=True)
graph_source = Source.from_file('network.gv')
graph_source

print(graph_source.source)

model.weights